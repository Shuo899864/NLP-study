# 为什么提出reformer
transformer的计算量和存储开销主要来源于：1.为了反向传播需要存储激活值，随层数线性增长；2.前馈层的维度远大于dmodel；3.注意力机制的计算和空间复杂度是L的平方。<br>
为了降低transformer的计算复杂度，reformer使用了：1.可逆层，可以让反向传播只存储一层的激活值；2.分块计算前馈层；3.局部敏感哈希将注意力复杂度从O(N2)降低到O(NlogN)。
# 局部敏感哈希注意力
## 存储高效的注意力
传统注意力是QK矩阵相乘，结果矩阵维度(length,length)可能会非常大，如果每次只计算一个q，然后在需要梯度的时候重新计算它，计算速度可能会变慢，但可以减少length倍的内存消耗。
## 哈希注意力
让Q和K来自同一个矩阵，Q=K，V单独用一个矩阵，由于softmax的结果主要集中在取值最大的元素上，因此不必对整个QK矩阵进行计算，只需要对每个q，在K中找到值与它最接近的一部分k，比如32或64个。
## 局部敏感哈希
哈希算法会让接近的向量计算出相近或者相同的结果，用它来找到最接近q的k。首先随机初始化一个矩阵R(dk,b/2)，然后计算h(x) = arg max(concat(xR,−xR))。
## 局部敏感哈希注意力
