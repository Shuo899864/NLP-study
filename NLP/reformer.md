# 为什么提出reformer
transformer的计算量和存储开销主要来源于：1.为了反向传播需要存储激活值，随层数线性增长；2.前馈层的维度远大于dmodel；3.注意力机制的计算和空间复杂度是L的平方。<br>
为了降低transformer的计算复杂度，reformer使用了：1.可逆层，可以让反向传播只存储一层的激活值；2.分块计算前馈层；3.局部敏感哈希将注意力复杂度从O(N2)降低到O(NlogN)。
# 局部敏感哈希注意力
## 存储高效的注意力
传统注意力是QK矩阵相乘，结果矩阵维度(length,length)可能会非常大，如果每次只计算一个q，然后在需要梯度的时候重新计算它，计算速度可能会变慢，但可以减少length倍的内存消耗。
## 哈希注意力
让Q和K来自同一个矩阵，Q=K，V单独用一个矩阵，由于softmax的结果主要集中在取值最大的元素上，因此不必对整个QK矩阵进行计算，只需要对每个q，在K中找到值与它最接近的一部分k，比如32或64个。
## 局部敏感哈希
哈希算法会让接近的向量计算出相近或者相同的结果，用它来找到最接近q的k。首先随机初始化一个矩阵R(dk,b/2)，然后计算h(x) = arg max(concat(xR,−xR))。
## 局部敏感哈希（LSH）注意力
首先，用哈希分桶。由于桶的大小可能不均匀，甚至可能一个桶中只有q没有k，所以首先对k进行处理，让其等于q的方向，然后，按桶排序，把同一个桶的放在一起。因为q=k，因此排序后的注意力会集中在对角线附近。<br>
分块：按每m个q进行分块，块内的q互相计算注意力，如果一个块的第一个q与前一个块属于一个桶，则要往前关注。
## 多轮LSH注意力
就是有可能相似的被分到不同的桶，为了减小这种几率，要多做几次哈希，最后将每轮结果并集得到需要计算注意力的集合。
## 禁止关注自身
由于qk是共享的，关注自身可以获得最大的注意力，所以修改了掩码方式，禁止关注自身，除非没有其他有效的注意目标。
# 可逆网络
如果不做任何处理，整个模型的参数量batch·sq_len·dmodel·layers（这里没算词嵌入层和前馈层）。可逆网络可以只保留最后一层，让Y1=X1+Attention(X2)，Y2=X2+FeedForward(Y1)，可以反推X2=Y2-FeedForward(Y1)，X1=Y1-Attention(X2)。这样存储复杂度中的layers被去掉。
## 前馈层分块
由于feed_forward层的维度远大于模型的维度。dff一般是4K。但是feed forward层又是和位置无关的，所以，可以分成c个块，每个块内单独计算。
