# 
## 全连接层
对输入做线性变换
```
torch.nn.Linear(in_features, out_features, bias=True)
```
输入形状：(N，in_features)<br>
输出形状：(N，out_features)<br>
参数形状：w:（out_features, in_features）b:(N, out_features)<br>
## 嵌入层
```
torch.nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False)
```
用于保存词嵌入，参数形状：（num_embeddings, embedding_dim）<br>
输入形状：(N, W)其中W为词典序号数字<br>
输出形状： (N, W, embedding_dim)<br>
## 卷积层
```
torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
```
输入输出形状：一维卷积(N,C_in,L_in),（N,C_out,L_out）二维卷积(N, C_in,H_in,W_in),（N,C_out,H_out,W_out）三维卷积(N,C_in,D_in,H_in,W_in),（N,C_out,D_out,H_out,W_out）<br>
kernel_size：卷积核大小<br>
stride：卷积核移动步长<br>
padding：边缘补0<br>
## 池化层
```
torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
torch.nn.MaxPool3d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
```
执行最大池化操作<br>
类似卷积层，但不带权重和偏置<br>
## 循环层
### RNN
```
torch.nn.RNN(input_size, hidden_size, num_layers, nonlinearity='tanh', bias=True, bidirectional=False)
```
输入形状： (batch, seq_len, input_size)<br>
输出形状：((batch, seq_len, hidden_size * num_directions),(batch, num_layers * num_directions, hidden_size))<br>
（RNN的步数即为seq_len，输出的是2元组，第一个是最后一层所有时刻的特征，第二个是所有层最后时刻的隐状态）<br>
input_size:输入特征维度，文本处理中词嵌入的维度。<br>
hidden_size：隐层的维度，内部全连接层的神经元数。<br>
num_layers：RNN堆叠的层数，多层时前一层的每一步h输出作为后一层的每一步x输入。<br>
nonlinearity：激活函数，'relu'或'tanh'。<br>
bidirectional：是否为双向RNN。<br>
### LSTM
```
torch.nn.LSTM(input_size, hidden_size, num_layers, bias=True, bidirectional=False)
```
输入形状： (batch, seq_len, input_size)<br>
输出形状：(batch, seq_len, hidden_size * num_directions),((batch, num_layers * num_directions, hidden_size),(batch, num_layers * num_directions, hidden_size))<br>
（基本同RNN，LSTM输出的第二位是一个包含h和c的元组）<br>
input_size:输入特征维度，文本处理中词嵌入的维度。<br>
hidden_size：隐层的维度，内部全连接层的神经元数。<br>
num_layers：LSTM堆叠的层数，多层时前一层的每一步h输出作为后一层的每一步x输入。<br>
bidirectional：是否为双向LSTM。<br>
## Dropout层
```
torch.nn.Dropout(p=0.5, inplace=False)
```
每次前向计算时随机地将输入元素置0。<br>
## 标准化层
### 批标准化
```
torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True)
torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)
torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True)
```
对每一个batch内的数据，计算均值和方差，进行标准化，然后重新缩放和平移。<br>
y=(x-mean[x])/(std(x)+e)*gamma+beta<br>
### 层标准化
```
paddle.nn.LayerNorm(normalized_shape, epsilon=1e-05, weight_attr=None, bias_attr=None, name=None)
```
对一个层内的神经元，计算均值和方差，进行标准化，然后重新缩放和平移。<br>
y=f((x-mean[x])/(std(x)+e)*gamma+beta)<br>
这里mean和std都是对一层神经元的数值进行计算，有激活函数之前和激活函数之后两种方式。<br>
## 激活层
### Relu
```
torch.nn.ReLU(inplace=False)
```
