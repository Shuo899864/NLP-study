## 全连接层
对输入做线性变换
```
torch.nn.Linear(in_features, out_features, bias=True)
```
输入形状：(N，in_features)
输出形状：(N，out_features)
参数形状：w:（out_features, in_features）b:(N, out_features)
## 卷积层
```
torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
```
输入输出形状：一维卷积(N,C_in,L_in),（N,C_out,L_out）二维卷积(N, C_in,H_in,W_in),（N,C_out,H_out,W_out）三维卷积(N,C_in,D_in,H_in,W_in),（N,C_out,D_out,H_out,W_out）
kernel_size：卷积核大小
stride：卷积核移动步长
padding：边缘补0
## 循环层
### RNN
```
torch.nn.RNN(input_size, hidden_size, num_layers, nonlinearity='tanh', bias=True, bidirectional=False)
```
输入形状： (batch, seq_len, input_size)
输出形状：((batch, seq_len, hidden_size * num_directions),(batch, num_layers * num_directions, hidden_size))
（RNN的步数即为seq_len，输出的是2元组，第一个是最后一层所有时刻的特征，第二个是所有层最后时刻的隐状态）
input_size:输入特征维度，文本处理中词嵌入的维度。
hidden_size：隐层的维度，内部全连接层的神经元数。
num_layers：RNN堆叠的层数，多层时前一层的每一步h输出作为后一层的每一步x输入。
nonlinearity：激活函数，'relu'或'tanh'。
bidirectional：是否为双向RNN。
### LSTM
```
torch.nn.LSTM(input_size, hidden_size, num_layers, bias=True, bidirectional=False)
```
输入形状： (batch, seq_len, input_size)
输出形状：(batch, seq_len, hidden_size * num_directions),((batch, num_layers * num_directions, hidden_size),(batch, num_layers * num_directions, hidden_size))
（基本同RNN，LSTM输出的第二位是一个包含h和c的元组）
input_size:输入特征维度，文本处理中词嵌入的维度。
hidden_size：隐层的维度，内部全连接层的神经元数。
num_layers：LSTM堆叠的层数，多层时前一层的每一步h输出作为后一层的每一步x输入。
bidirectional：是否为双向LSTM。
