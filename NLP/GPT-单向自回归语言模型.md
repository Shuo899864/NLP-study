# 为什么提出GPT
利用标注数据进行训练的代价是很大的，因此可以利用无标签数据训练获得文本表示，进而加速带标签数据的训练。但这里有两个问题，一是如何选择优化目标也就是预训练任务，二是如何把学到的文本表示迁移到目标任务。作者提出了GPT，可以学习文本的通用表示，并且只需要少量的调整即可对很多任务进行迁移.
# GPT的结构
## 无监督预训练阶段
这里是使用了标准的语言模型，最大化对数似然函数L1(U) = Σilog P(ui|ui−k, . . . , ui−1; Θ)。GPT使用了transformer的decoder（不含交互注意力层），输入是要预测内容的上文，经过词嵌入层并加入位置嵌入，再经过decoder，最后经过词嵌入层和softmax输出P(U)。因为这里输入是上文一个窗口内的token，输出是预测的下一个token，所以使用词嵌入层作为最后的decoder。
## 微调阶段
微调阶段其实差不多，只是输入变成了一串token(x1,x2,...,xm)，输出变成了各自任务的输出y，因此最大化似然函数变成了L2(C) = Σ(x,y)log P(y|x1, . . . , xm)，因此这里最后一层的decoder变成了与具体任务有关的线性层。作者这里还发现在微调阶段结合语言模型可以加速收敛和提升模型效果，因此最后使用的似然函数L3(C) = L2(C) + λ ∗ L1(C)
## 特定任务的输入转换
文本蕴含：前提和假设连接在一起，用一个特殊符号分隔。<br>
文本相似度：因为两端文本没有顺序关系，因此text1+text2和text2+text1分别输入一次，把两次的输出相加作为最后线性层的输入。<br>
问答和推理：把题干，问题放在一起，然后分别与每个回答构成一个输入{z,q,$,ak},最后所有的输出做softmax。
# 实验
1.模型层数越多效果越好<br>
2.微调阶段加入语言模型可以提升大数据集上的效果，但是在小数据集上反而效果不佳
