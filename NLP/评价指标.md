## Accuracy
准确率，预测标签正确的比例。<br>
对于二分类：acc=(TP+TN)/(P+N)<br>
对于多分类，acc=正确的标签在topk个预测值的个数（k取1时为预测值为正确标签的个数）/总样本个数
多标签分类的准确率：acc=每个样本被正确分类标签个数的和/（总样本个数*标签数）
## Precision
精确率，预测为正样例中实际为正的比例。<br>
p=TP/(TP+FP)
## Recall
召回率，实际为正样例中预测为正的比例。<br>
r=TP/(TP+FN)
## F1 score
精确率和召回率的调和平均数，兼顾了二者。<br>
F1=2/(1/p+1/r)=2pr/(p+r)<br>
## 真正例率(TPR)、假正例率(FPR)
TPR=TP/(TP+FN)
FPR=FP/(FP+TN)
## ROC曲线、AUC
ROC：横轴FPR，纵轴TPR<br>
绘制方法：1.得到模型对于所有样本的预测分数并从大到小排序。<br>
2.把分类阈值设置为1，即把所有样本分类成负样本，得到点(0,0)。<br>
3.减小阈值，使得排序后的样本每次有一个被分类为正样本，计算FPR和TPR并在图上标出。<br>
4.当所有样本被分类为负样本，得到点(1,1)
AUC值即为ROC曲线与横轴围成的面积，ROC曲线越接近y=x，表明对于正负样本越没有区分力，此时AUC=0.5。<br>
我们希望模型的ROC曲线在y=x上方，并且越高越好，也就是TPR接近1，FPR接近0最好，此时0.5<AUC<=1。<br>
反之，如果ROC曲线在y=x下方，说明模型容易把正样本分类为负样本，把负样本分类为正样本，此时0<=AUC<0.5，但这种情况下把模型的输出结果反过来就好，所以一般情况下默认ROC曲线在y=x上方，AUC的最小值为0.5<br>
