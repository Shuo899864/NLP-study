# 为什么提出longformer
self-attention的时间复杂度和空间复杂度是序列长度的平方，处理长文本时通常采用分割等方法，会损失分段之间的信息。<br>
longformer使用加窗的局部上下文注意力和目标任务启发的全局注意力对特定任务产生一个偏置。
# 模型
## 滑动窗口
就是类似bigbird，每次只关注窗口w内的token，使复杂度降低到wn，感受野大小为lw，对于每层可以使用不同的w，以平衡效率和模型的表达能力。
## 加宽滑动窗口
窗口变得稀疏，每隔d计算一次，感受野大小为ldw。在多头注意力里，可以让某些头具有间隔，某些头不带间隔，以此分别获取局部信息和更大范围的上下文。
## 全局注意力
以上两种方式对于学习特定任务的表示不够灵活，类似bigbird，选取几个特定位置的token，使其可以attend到所有token，并且所有token可以attend到几个特定token。<br>
global token的意义是可以提供全局信息，在文本分类中类似[CLS]的功能，在QA中global token是所有的问题token。
## 使用两组映射
一组用来计算滑动窗口注意力，一组计算全局注意力
## 实现方式
loop：存储高效的pytorch实现，速度很慢；<br>
chunks：只支持非加宽的情景<br>
cuda：基于cuda的高效实现
# 实验
使用滑动窗口，在低层使用小的宽度，在高层使用大的宽度，在低层不使用加宽，在高层使用逐渐加宽，但是仅在两个注意力头上使用加宽。<br>
实验证明逐层增加的w比逐层降低的w/固定w效果好，在两个注意力头使用加宽比不使用加宽效果好。
