# 为什么提出BERT
简单来讲，目前有两种预训练模型：基于特征的和基于微调的。基于特征的比如ELMO，使用包含预训练表示的特定的结构作为额外特征，基于微调的比如GPT，引入最小的任务特定参数，通过微调对下游任务进行训练，但是它们都没有使用双向语言模型，比如GPT，使用了单向的transformer，因此限制了一些任务的发挥，比如QA。因此BERT提出了一种双向语言模型，使用MLM（masked language model）进行训练，通过随机mask一个token然后预测它，可以结合双向语义信息。除此之外还使用了NSP（next sentence prediction）训练成对句子的表示。<br>
所以BERT主要的贡献一是提出了双向语言模型，二是证明了预训练的语言模型比人工特征工程效果好。<br>
# BERT的实现细节
作为预训练模型，BERT首先在无标签样本上进行预训练，在具体任务上进行微调时，首先加载预训练的模型参数，然后使用有标签样本在下游任务上进行微调。<br>
## 模型结构
主要是多层的transformer encoder。<br>
## 输入和输出
输入主要是一个30000词的词嵌入层，并且每个输入的样本开头都会加一个【CLS】的特殊token，在输出时这个token代表整个句子的聚合语义信息，可以用来做分类任务。另外还有一个【SEP】的特殊token，输入是句子对的时候两个句子用【SEP】连接。<br>
Bert的输入由三部分相加得到：词嵌入，segment embedding（用来区分句子对的两个句子，比如句子A全是0，句子B全是1），position embedding（用来区分词语位置，由0,1,2,3……进行初始化，然后学习而来）<br>
# BERT的训练
## MLM
随机MASK掉一个句子15%的token，然后让模型预测被MASK掉的token。但是由于MASK在微调阶段并不会出现，会造成预训练和微调的不匹配，因此在替换的时候，80%的token替换成【MASK】，10%的token替换成一个随机token，10%的token保持不变。然后拿这个token的输出Ti接一个softmax使用交叉熵进行预测。
## NSP
从训练样本中抽取句子对A和B，其中B有50%是A的下一句，50%是从其他地方随机抽取，然后把【CLS】的输出拿出来进行预测。
# BERT的微调
微调主要是具体任务的输入输出，比如QA，把问题-段落作为句子对输入，输出的是每一个token是答案的开头/结尾的概率，文本分类，把单个句子输入，使用CLS位置的输出进行分类。
