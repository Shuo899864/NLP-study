## SGD随机梯度下降
w=w-lr*g<br>
每次将一个batch的一部分计算梯度，极端情况每次只选取一个数据计算梯度。<br>
好处：减少计算量。<br>
问题：学习率对结果影响大；下降方向容易震荡；容易陷入鞍点。<br>
## momentum
v=uv+g<br>
w=w-lr*v<br>
不仅会用当前时刻梯度还会用历史梯度<br>
好处：抵消震荡，下降更平滑，不容易陷入鞍点。<br>
## momentum with nesterov
v=uv+g<br>
v'=uv+g<br>
w=w-lr*v'<br>
梯度是基于动量先计算下一步的位置，好处是可以提前纠正动量方向的偏差。<br>
## Adagrad
r=r+g^2<br>
w=w-lr*g/(r^0.5+e)<br>
自适应梯度，可以为不同参数分配不同的学习率。参数梯度大则学习率小，参数梯度小则学习率大。可以减少震荡。<br>
问题：可能出现学习率下降过快。<br>
## RMSProp
r=p*r+(1-p)g^2<br>
w=w-lr*g/(r^0.5+e)<br>
在学习率调节系数上加了移动平均，缓解学习率下降过快的问题。p的典型值为0.9。<br>
## Adam
r=p*r+(1-p)g^2<br>
s=q*s+(1-q)g<br>
lr=lr*(1-p)^0.5/(1-q)<br>
w=w-lr*s/(r^0.5+e)<br>
能够利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。典型值p=0.999，q=0.9<br>
## AdamW
r=p*r+(1-p)g^2<br>
s=q*s+(1-q)g<br>
lr=lr*(1-p)^0.5/(1-q)<br>
w=w-lr*(s/(r^0.5+e)+l*w)<br>
用来解决Adam优化器中L2正则化失效的问题。
