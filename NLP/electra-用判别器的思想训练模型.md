# 为什么提出electra
BERT会把输入的token随机MASK一部分，让模型还原被MASK的token，但是这样训练数据不能被充分利用，只能学习15%的东西。electra提出了一个检测token是否被人工生成的token替换的任务，有点类似对抗训练，训练一个小的MLM用来人工生成token，作为generator，我们的模型作为discriminator，判断这个token是不是被人工生成的。这种方法比bert有更高的精度并且训练更快。
# 模型
模型包含两部分，G和D，G通常是一个小的MLM，用来编码文本，首先随机生成mask位置，使用一个多分类线性层来对每一个mask的位置预测输出；D同样先编码文本，然后对于每个位置使用一个二分类线性层预测其是否被替换。这里与GAN不同的是，G的损失函数是最大似然，D的损失函数是交叉熵，让G尽可能预测正确的输出，而不是让D无法辨别。因为如果按照GAN的方式去训练，MLM的采样过程是无法反向传播的，所以G和D的损失是单独计算。整个训练过程的损失函数是G和D各自损失的加权和。
# 权重共享
因为G比D要小，因此只共享词嵌入层，实验证明共享嵌入层效果比不共享要好，因为G的softmax函数可以更新所有词的嵌入。
# 模型大小
这里使用一个小的G原因是可以减少计算量，作者发现G的参数量是D的1/4-1/2的时候效果最好，如果G的参数更多的话D分辨就会比较困难。
# 训练算法
作者还是用了其他训练算法：<br>
两步法：使用以下两步训练：1.只训练G一定的步数；2.使用G的参数初始化D的参数（需要G和D一样的大小）。训练D一定的步数，这期间固定G的参数。两步训练中从G切换到D后下游任务效果明显提高，但是不如同步训练。<br>
对抗训练。效果不如最大似然训练。
