# 为什么提出XLNET
基于自编码的BERT模型可以利用双向语言信息，效果比基于自回归的模型好，但是BERT简单地假设MASK之间相互独立，忽略了MASK位置间的依赖关系，并且预训练和微调阶段存在差异（预训练的MASK在微调中并不存在）。作者提出XLNET，一个能学习双向上下文的自回归模型。
# XLNET的结构
基于AR和AE两种模型各自的优缺点，提出了permutation language modeling，想法就是对于长度为T的序列，有T!个排列，每次选取一个排列进行优化，因为所有排列共享一套模型参数，因此对于每个token，实际上可以看到左右两边所有的token。这里的排列并不是通过打乱序列顺序实现的，而是通过注意力掩码实现的，因为在微调过程中模型只会看到自然顺序的文本。
## 注意力掩码和双流自注意力
为了模拟出不同的排列，在原始自注意力上使用了掩码，比如序列3241，在预测2的时候可以看到3，预测4的时候可以看到32，预测1的时候可以看到324.<br>
但是这里有一个问题，预测的时候只能看到前面内容的state，而看不到预测的位置信息，这样只要前面的state相同，不同的位置的预测可能是相同的，这显然违反常识。因此这里不光要有内容信息，还要有位置信息，这里使用了双流自注意力，一个h代表内容信息，一个g代表位置信息，对于一个token，当预测它自己的时候，只提供其位置信息，不提供内容信息，而使用它预测其他token的时候，不仅提供位置信息，也提供内容信息。<br>
在第一层初始化的时候，h使用词嵌入，g使用随机初始化的参数w。对于每一层位置t，gt的Q是上一层的gt，KV是上一层h<t，h的Q是上一层的ht,KV是上一层的h<=t。gt可以看到t的位置信息，但看不到内容信息，ht则是能看到t的位置信息和内容信息。<br>
在最后预测的时候，使用的是g而不是h。
## 部分预测
为了降低收敛的难度，把每个句子分成两部分，使用前一部分去预测后一部分，使用一个K的参数，使得被预测部分是句子总长度的1/K。
## 使用transformer-XL的记忆
如果有两段文字，模型先处理第一段，然后把state放在缓存中，在处理第二段时可以把第二段的state跟缓存中第一段的state concat起来.
## 相对分段编码
如果ij时来自同一段，则sij=s+，否则sij=s-，s+和s-都是学习出来的参数。计算注意力的时候aij = (qi + b)Tsij，q来自于原始注意力的q，b是可学习的偏置，最后把aij加到原始注意力权重上。这样做的好处是提高生成任务的效果，并且可以处理多段输入。
