# 为什么提出ernie-gen
目前的自然语言生成模型的主要缺陷是在teacher-forcing训练中，模型每次输入的是正确的token，无论它生成的是否正确，而在推理过程中前一个错误的生成会影响到后面，导致错误累积。ernie-gen使用了一个重新设计的注意力机制，因此能缓解上述问题。
# 模型结构
## 多粒度目标片段
给定一段长度的文本S，使用一个概率分布循环地在原文本中采样出不同长度的文本，达到一个比例后停止采样（这里设置成S的25%），然后把采样出的文本拼接在一起得到目标序列T，被采样过后剩下的原文本为S'，模型的优化目标是最小化似然函数-logP(T/S')，这里使用了一个混合分布D = {U(1, 4), U(4, 32)}概率分别是0.4和0.6，让模型可以按不同粒度采样，短片段可以让模型学到词语间的语义关系，长片段可以让模型学到句子级表达。
## 能意识到噪声的生成
为了让模型在生成过程中有一定的错误检测能力，在上一步采样的T的基础上，随机的替换词语得到T'。
## 多流注意力
