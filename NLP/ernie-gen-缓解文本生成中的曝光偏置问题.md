# 为什么提出ernie-gen
目前的自然语言生成模型的主要缺陷是在teacher-forcing训练中，存在曝光偏置问题：训练时模型每次输入的是正确的token，无论它生成的是否正确，而在推理过程中每次输入的是前一个生成的token，前一个错误的生成会影响到后面，导致错误累积。而且人类在写作中往往不是一个字一个字生成的，因此传统的训练方式可能会过度依赖上一个字。ernie-gen使用了一个重新设计的注意力机制，因此能缓解上述问题。
# 模型结构
## 多粒度目标片段
给定一段长度的文本S，使用一个概率分布循环地在原文本中采样出不同长度的文本，达到一个比例后停止采样（这里设置成S的25%），然后把采样出的文本拼接在一起得到目标序列T，被采样过后剩下的原文本为S'，模型的优化目标是最小化似然函数-logP(T/S')，这里使用了一个混合分布D = {U(1, 4), U(4, 32)}概率分别是0.4和0.6，让模型可以按不同粒度采样，短片段可以让模型学到词语间的语义关系，长片段可以让模型学到句子级表达。
## 噪声感知的生成
为了让模型在生成过程中有一定的错误检测能力，在上一步采样的T的基础上，随机的替换词语得到T'。
## 多流注意力
### 上下文流
就是原始transformer里encoder和decoder的注意力，encoder能看到所有输入文本，decoder只能看到encoder输入的文本和decoder已经生成的文本。
### 填充生成机制
通过在decoder每次生成后面填充一个特殊符号ATTN去汇聚上文信息，使用ATTN而不是当前生成的字符去预测下一个字符，减弱了解码时对上一个字符的过度依赖，有利于缓解曝光偏置。
### 逐字生成流
在填充生成机制的基础上，把所有符号合并起来构成一个序列AW，并且ATTN看不到相同位置的字符。
### 逐段生成流
与逐字生成流不同的是，给定一个片段长度的列表，ATTN看不到同一个片段内的字符。作者不想通过随机采样来生成片段，于是采用T检验的方法：<br>
1.基于初始假设 : “一个随机的n-gram不是一个具备统计意义的片段“，可以计算得到训练数据中所有2-gram和3-gram的t-统计值
2.选取t-统计值最高的20W个2-gram和5W个3-gram和所有的1-gram作为片段词典
3.按照词典中3gram-2gram-1gram的顺序去采样片段，直到一个片段在词典中被找到
### 多流注意力
相当于分别拿上下文流和AW以及AS进行训练，最后损失是两个流的加权和，在预训练阶段两个任务权重都是0.5，微调阶段只训练逐字生成任务。
### 填充解码
在解码过程中，使用ATTN汇聚上文信息，但是在预测一个字符完成后，会把当前ATTN从序列中去除。
