# 为什么提出roformer
RNN通过循环计算隐状态编码序列顺序，transformer模型的自注意力机制不考虑顺序，因此通过其他方式引入位置信息，一种是相对位置编码，一种是绝对位置编码，往往是与输入的维度相加，roformer提出一种新的位置编码，是旋转矩阵与输入相乘。
# RoPE
f(q,m)=qe^imθ，转换成矩阵形式[[cosmθ,-sinmθ],[sinmθ,cosmθ]][q0,q1](二维情况)，对于任意偶数维，可以写成二维矩阵的拼接
