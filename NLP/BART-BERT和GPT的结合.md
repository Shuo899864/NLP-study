# 为什么提出BART
BART使用任意噪声函数破坏文本，然后训练模型重建文本。使用了两种加扰方式：随机打乱原文的顺序和随机MASK一段文本。简单来讲，BART就是BERT和GPT的结合，BERT是双向的encoder，所以BERT不适合做生成任务，GPT是从左到右的decoder，GPT不能学到双向的语义信息。
# 模型结构
BART使用双向encoder和单向decoder，输入是加扰文本，输出是原文本。模型结构与原版transformer相同，只是RELU激活函数换成了GELU，并且权重使用(0,0,02)的正态随机初始化。BART的encoder和decoder各有6层，与BERT不同的是，（1）encoder和decoder之间有交互注意力层，（2）去掉了BERT预测单词时的线性层。
# BART预训练
使用decoder输出文本和原文本之间的交叉熵作为目标损失函数进行训练。BART试验了几种加扰方式：<br>
（1）token级别的MASK：与BERT的方法相同。<br>
（2）随机删除token：随机删除token让模型决定哪些位置缺少token.<br>
（3）文本注入：将使用泊松分布（λ=3）生成的片段长度替换成MASK，片段长度可以为0，代表插入一个MASK。可以让模型预测一个片段有多少token缺失。<br>
（4）句子置换：使用句号把文本分割成句子，随机打乱这些句子的顺序。
（5）文本旋转：随机选择一个token，将其作为句子开头对句子进行旋转。
# BART微调
## 文本分类
文本分别输入encoder和decoder，使用decoder最后一个token作为整个句子的信息，输入全连接层进行分类。
## token分类
文本分别输入encoder和decoder，使用decoder输出的每个token的state作为每个token的信息。
## 序列生成
跟预训练任务一样，文本输入encoder（这里说输入文本是加了干扰的），然后decoder可以直接输出目标文本。
## 机器翻译
这里首先说之前的研究表明机器翻译可以利用预训练的encoder，但是难利用预训练的decoder，作者提出可以将BART作为预训练decoder，前面接一个额外的encoder，将BART encoder的embedding层替换成一个新的随机初始化的encoder，然后端到端地训练，这样新的encoder可以将外语编码成BART的输入。<br>
训练是分两步训练：首先冻结大部分的BART的参数，然后训练新的encoder的参数，以及位置嵌入/BART encoder第一层的自注意力层的输入映射矩阵的参数。第二步，将模型的所有参数训练少量的步数。
# 实验结果
比较了几种语言模型和几种加扰方法的效果。语言模型有：（1）GPT的从左到右语言模型，（2）XLNET的排列语言模型，（3）BERT的MLM，（4）UniLM的多任务MLM，（5）MASS的MASK seq2seq。<br>
首先是几种加扰方法的对比，发现（4）（5）句子置换和文本旋转的效果一般，另外三种效果比较好。<br>
（2）和（3）对于生成任务效果不好，因为预训练时没有生成任务。<br>
单向语言模型（1）在SQUAD问答数据集上表现不好，因为只能看前面的上下文不能看后面的上下文。<br>
预训练任务不是影响模型效果的唯一因素，模型结构的优化也有帮助。<br>
单向语言模型（1）在一个数据集上超过了BART，说明当数据的输入对输出限制不强的时候，BART的效果差一点。<br>
除了上面一个数据集，BART在其他数据集上超过了其他语言模型。结论就是BART效果比较好，在判别任务上接近roberta，在多个生成任务上取得了SOTA。
