# 为什么提出ernie2.0
就是跟ernie一样提出模型可以学习词汇句法语义信息，并且还提出一个模仿人类持续学习的概念。
# 模型架构
主要是讲如何持续学习，有三种方法：第一种是持续多任务学习，就是先学习task1，然后学习task1和task2，以此类推，但是每轮学习的数据量是不同的，比如task1最先引入，就一开始少学一点，后面慢慢学，taskn最后引入，就一次性学完；第二种是多任务学习，就是把所有任务一起学习，一起优化loss；第三种是持续学习，每次训练一个任务，可能会忘记学到的东西。<br>
在多任务学习中，多个任务共用一个encoder，可以是RNN或者transformer，包含两种损失函数：词语级别和句子级别。每个任务都有自己的损失函数。
## 模型结构
encoder跟之前一样，这里引入了task embedding，类似segment embedding，根据任务不同从0到N，因此这里的输入包含四种embedding。
## 预训练任务
### 词语级别任务
一个是ernie1.0里面，将MASK的短语/实体恢复出来的任务；一个是大写词语预测的任务，用于捕捉重要的命名实体；一个是预测词语是否在文档其他段中出现的任务，用于捕捉文档的关键字。
### 句法级别任务
一个是把句子打乱然后恢复原来顺序的任务，用来学习句子间的关系；一个是预测句子关系，有三种关系相邻/同一篇文档但不相邻/不同文档。
### 语义级别任务
句子语义关系预测；信息检索任务，用来预测短的查询和候选文本的相关性。
# 实验
比较有意思的是三种持续学习的对比，持续学习效果最差，多任务学习好一点，持续多任务学习效果最好。
