# 为什么提出distillbert
随着transformer的提出，大模型已经成为趋势，但是也带来了一些问题，比如训练一个模型需要很多资源，以及大模型限制了可用的设备范围。所以用蒸馏的方法训练了一个更小的bert。
# 知识蒸馏
就是用大模型作为教师，小模型作为学生，训练小模型让它可以重复大模型的输出，使用教师模型输出的概率值作为软标签，输出要经过一个带温度的softmax（就是softmax的每一项都除以一个温度系数T，在推理阶段T=1），训练过程融合了三种损失：使用Σtlogs作为蒸馏损失Lce，使用MLM的损失Lmlm，以及学生模型和教师模型输出state之间的cos相似度Lcos。
# 模型
## 学生模型
学生模型是一个小的bert，encoder的层数被缩减到一半，并且移除了token type embedding和pooler，使用教师模型的参数初始化学生模型的参数，具体是每两层参数取出一层用来初始化学生模型的参数。
## 蒸馏
使用大batch size，使用动态MASK，去掉NSP。
# 实验
distillbert保留了97%的bert性能，并且在下游任务中的性能损失也不多，相比bert-base减少了30%多的参数量和推理时间。
