# 为什么提出albert
大模型预训练的效果是很好的，但是内存和训练速度对大模型的训练构成了限制。因此ALBERT是一个轻量化的BERT，ALBERT主要包含两个改进：一是把巨大的词嵌入矩阵分解成两个较小的矩阵，二是跨层参数共享。相比于BERT-large，ALBERT的参数量少18倍，训练速度快1.7倍。ALBET还提出了SOP的预训练任务，解决了BERT的NSP任务中存在的问题。
# albert的结构
## 分离词嵌入维度和隐层维度
作者认为词嵌入维度代表着单词级别的语义表示，而隐层代表整个句子的语义表示，因此词嵌入维度没有必要那么大，可以把原先的词嵌入V×H分解成两个矩阵的相乘V×E，E×H，这样模型的参数量就会大大降低。
## 跨层参数共享
有两种参数共享的方式：共享前馈神经网络和共享注意力。ALBERT默认是都共享的。共享部分的权重是一起训练的。
## 跨句子相关性任务
作者发现BERT的NSP任务效果是不好的，原因是NSP任务相比MLM太简单了，NSP任务的两个句子是同一片段抽取的，因此模型更倾向于学习句子主题，而不是句子顺序。ALBERT换成了SOP任务，其中正样本是两个连续的句子，负样本是正样本的句子交换顺序。这能强迫模型学习句子对之间更细的区别。
## 其他细节
增加样本长度可以提升效果，因此样本都是以句子对的形式输入的，也就是同一个句子重复两次。还有就是同时MASK n-gram，论文里设置的n是3
# 结论
预训练的时候，数据填充的更满，到512这种，有利于提升模型效果。<br>
mask n-gram有利于提升效果。<br>
词向量矩阵分解能减少参数，但是也会降低性能<br>
跨层参数分享可以降低参数，也会降低性能，通过实验图知道，attention共享效果还好，FFN共享效果降低有点多（FFN学到的东西比较重要？）<br>
取消NSP，使用SOP，正负样本来自同一个文档，但是顺序不同。<br>
推理速度来看，同等规格，ALBERT速度确实变快，但是并不明显，同等效果，速度变慢。
